# -*- coding: utf-8 -*-
"""vae_mc3.ipynb
0425: change loss function (*latent_dim)
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_LKG42SnWDfjyqqhn-QKF76y4AyrGBcD
"""

import numpy as np
from keras.layers import Input, Dense, Lambda, Concatenate, Layer, Add, Multiply
from keras.models import Model, Sequential
from keras.optimizers import adam
from keras.losses import mse, mae
#from sklearn.model_selection import train_test_split #cross_validation
#from sklearn import preprocessing
#import pandas as pd
from keras import backend as K
#import matplotlib.pyplot as plt
#from functools import partial
from keras.callbacks import Callback
from keras import regularizers

## read in parameter
import sys
mc_samples = int(sys.argv[1])
#N = int(sys.argv[2])
#random_state = int(sys.argv[3])
suffix = sys.argv[2] #"1_16_0.2_0.1_1"
prefix = sys.argv[3] # "simulation_GP"
layer = int(sys.argv[4]) #7
nodes = int(sys.argv[5]) #32
layer2 = int(sys.argv[6]) #3
nodes2 = int(sys.argv[7]) #16
repeat = sys.argv[8] # 1-5
noise = float(sys.argv[9])**2
log_sigma2 = np.log(noise)

batch_size = 512
epochs = 400
epsilon_std = 1.0
#noise = 0.1**2 # for x
#prior_mean = 0
#prior_var = 4
regu = regu1 = 1e-5
laplace = False

results = np.zeros((5, 7)) # NN_val_err, NN_test_ise, train_ise, val_ise, train_ll, val_ll, test_ise

file1 = prefix + "_train_" + suffix + "_" + sys.argv[9] + "_" + sys.argv[8] + ".txt"
file2 = prefix + "_test_" + suffix + "_0.1_" + sys.argv[8] + ".txt"
latent_dim = 2

train_dat = np.loadtxt(file1) #simulation_GP_train_1_16_0.2_0.1.txt
test_dat = np.loadtxt(file2) #np.concatenate((train_dat[2500:,0:latent_dim], train_dat[2500:, (2*latent_dim+1): (2*latent_dim+2)]), axis=1)#

#idx = (test_dat[:,0] > 0.2) * (test_dat[:,1] < -0.2)!=1
#test_dat = test_dat[idx,:]

#idx = (test_dat[:,0] < -0.5) * (test_dat[:,1] > 0.5)!=1
#test_dat = test_dat[idx,:]

def build_model(layer, nodes, activ ='relu', input_dim = 1, output_dim = 1, regu = -1, alpha = 0.3):
    model = Sequential()
    if regu > 0:
        model.add(Dense(nodes, input_dim=input_dim, activation=activ, 
                        kernel_regularizer=regularizers.l2(regu), bias_regularizer=regularizers.l2(regu))) #, kernel_initializer='he_normal', bias_initializer='he_normal'))
        for l in np.arange(layer):
            model.add(Dense(nodes, input_dim=nodes, activation=activ, 
                            kernel_regularizer=regularizers.l2(regu), bias_regularizer=regularizers.l2(regu)))#, kernel_initializer='he_normal', bias_initializer='he_normal'))
        model.add(Dense(output_dim, input_dim=nodes, kernel_regularizer=regularizers.l2(regu)))
    elif activ == 'leakyrelu':
        model.add(Dense(nodes, input_dim=input_dim))
        model.add(LeakyReLU(alpha = alpha))
        for l in np.arange(layer):
            model.add(Dense(nodes, input_dim=nodes))
            model.add(LeakyReLU(alpha = alpha))
        model.add(Dense(output_dim, input_dim=nodes))
    else:
        model.add(Dense(nodes, input_dim=input_dim, activation=activ)) 
        for l in np.arange(layer):
            model.add(Dense(nodes, input_dim=nodes, activation=activ))
        model.add(Dense(output_dim, input_dim=nodes))
   
    return model

## prior model ##
def prior_model(layers, nodes, latent_dim2, activ ='relu', regu = -1):
    x1 = Input(shape=(latent_dim2,))
    x2 = Input(shape=(latent_dim2,))
    for l in np.arange(layers):
        px1_shift = build_model(1, nodes , activ='relu', regu = regu, input_dim = latent_dim2, output_dim = latent_dim2) 
        px1_log_scale = build_model(1, nodes , activ='relu', regu = regu, input_dim = latent_dim2, output_dim = latent_dim2)
        if l == 0:
            z_shift = px1_shift(x2)
            z_scale = px1_log_scale(x2)
            z1 = Add()([x1, z_shift])          
        elif l % 2:
            z_shift = px1_shift(z1)
            z_scale = px1_log_scale(z1)
            if l ==1:
                z2 = Add()([x2, z_shift]) 
            else:
                z2 = Add()([z2, z_shift]) 
        else:
            z_shift = px1_shift(z2)
            z_scale = px1_log_scale(z2)
            z1 = Add()([z1, z_shift]) 

        if l==0:
            log_det = z_scale
        else:
            log_det = Add()([log_det, z_scale])

        z_scale = Lambda(lambda t: K.exp(t))(z_scale)
        if l% 2:
            z2 = Multiply()([z2, z_scale])
        else:
            z1 = Multiply()([z1, z_scale])
    nz = Concatenate()([z1, z2]) # normal density


    return Model(inputs = [x1, x2], outputs = [nz, log_det])



def priorLoss(yTrue, yPred):
    prior_loss = K.sum(K.square(nz)/2 - log_det, axis = -1)
    return K.mean(prior_loss)



class changeNoise(Callback):
  def __init__(self, noisey):
      super(changeNoise, self).__init__()
      self.noisey = noisey 

  def on_epoch_end(self, epoch, logs={}):
      #print("Setting noisey to =", str(K.get_value(self.noisey)))
      if epoch > 10:   
          K.set_value(self.noisey, logs.get('mise2'))

class LossLayer(Layer):

    def __init__(self, *args, **kwargs):
        self.is_placeholder = True
        super(LossLayer, self).__init__(*args, **kwargs)

    def call(self, inputs):

        mu, log_var, z, fz, w, y, nz, log_det = inputs #
       
        w = K.expand_dims(w, axis = 1)
        y = K.expand_dims(y, axis = 1)
        
        if laplace:
            reconstruction_loss = K.sum(K.square(y - fz), axis=-1)/noisey/2 + K.sum(K.abs(w - z), axis=-1) /noise
        else:
            reconstruction_loss = K.sum(K.square(y - fz), axis=-1)/noisey/2 + K.sum(K.square(w - z), axis=-1) /noise/2
       
        prior_loss = K.sum(K.square(nz), axis = -1) /2 - K.sum(log_det, axis = -1) #
        
        post_loss = .5 * (K.square(mu - z) /K.exp(log_var) + log_var)
        post_loss = K.sum(post_loss, axis=-1)
      
        return  reconstruction_loss + prior_loss - post_loss


class WeightLayer(Layer):

    def __init__(self, *args, **kwargs):
        self.is_placeholder = True
        super(WeightLayer, self).__init__(*args, **kwargs)

    def call(self, loss):
        
        log_weight = K.stop_gradient(-loss)
        
        log_weight -= K.max(log_weight,axis = 1,keepdims= True)
        
        weight = K.exp(log_weight)
        weight = weight/K.sum(weight,axis = 1,keepdims= True)
        

        return  weight

for i, fold_size in enumerate([125, 250, 500, 1000,2000]): # [50, 125, 250, 500] for mixed kernel
  for f in [4]: #np.arange(5):
    X_train = np.concatenate((train_dat[0: f*fold_size,:], train_dat[(f+1)*fold_size:5*fold_size,:]))
    X_val = train_dat[f*fold_size : (f+1)*fold_size,:]

    K.clear_session()

    noisey =  K.variable(0.1)
    noiseparam = changeNoise(noisey)
    
    val_err = 1e8
    # initialization by direct regression model (repeat 3 times, get the smallest validation error)
    for kk in np.arange(5):
      model0 = build_model(layer, nodes, input_dim = latent_dim, regu = regu)
      model0.compile(loss=mse,optimizer='adam')  

      history = model0.fit( X_train[:,latent_dim : (2*latent_dim)] , X_train[:,2*latent_dim],
          batch_size=np.min([1024,X_train.shape[0]]),epochs=500,verbose=0, shuffle=True)

      temp = np.mean((model0.predict(X_val[:,latent_dim : (2*latent_dim)]).transpose() - X_val[:,2*latent_dim])**2) #history.history['loss'][-1]
      if temp < val_err:
          val_err = temp
          org_weights = model0.get_weights()
    
    model0.set_weights(org_weights)
    #results[i, 1] = history.history['val_loss'][-1]
    results[i, 0] += val_err #np.mean((model0.predict(X_val[:,latent_dim : (2*latent_dim)]).transpose() - X_val[:,2*latent_dim])**2) #history.history['loss'][-1]

    model00_predict = model0.predict(test_dat[:,0:latent_dim])
    results[i, 1] += np.mean((model00_predict.transpose() - test_dat[:,latent_dim])**2)


    # initialize prior model
    latent_dim2 = int(latent_dim/2)
    pm0 = prior_model(3,32,latent_dim2)
    z1 = Input(shape=(latent_dim2,))
    z2 = Input(shape=(latent_dim2,))
    nz, log_det = pm0([z1,z2])
    pm2 = Model(inputs = [z1, z2], outputs = [nz, log_det])

    pm2.compile(optimizer='adam', loss=priorLoss) 

    history = pm2.fit(
      [X_train[:,latent_dim:(latent_dim+latent_dim2)],X_train[:,(latent_dim+latent_dim2):(2*latent_dim)]],
      [X_train[:,latent_dim:(2*latent_dim)],X_train[:,latent_dim:(2*latent_dim)]],
      #validation_data = ([test_dat[:,0:1],test_dat[:,1:2]], [test_dat[:,0:1],test_dat[:,1:2]]),
      shuffle=True,
      epochs=300,
      verbose = 0, 
      batch_size=np.min([512,X_train.shape[0]])
    )


    x = Input(shape=(latent_dim + 1,))
    x1 = Input(shape=(latent_dim,))
    y1 = Input(shape=(1,))

    #h = Dense(nodes2, activation='relu')(x)

    model_mu = build_model(layer2, nodes2, regu = regu1, input_dim = latent_dim + 1, output_dim = latent_dim)
    z_mu = model_mu(x)
    z_mu = Add()([z_mu, x1])

    model_var = build_model(layer2, nodes2, regu = regu1, input_dim = latent_dim + 1, output_dim = latent_dim)
    z_log_var = model_var(x)
    #z_log_var = Lambda(lambda t: t + log_sigma2)(z_log_var)

    encoder = Model([x,x1], [z_mu,z_log_var])

    z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)

    eps = Input(tensor=K.random_normal(stddev=epsilon_std,
                                       shape=(K.shape(x)[0],
                                              mc_samples,
                                              latent_dim)))
    z_eps = Multiply()([z_sigma, eps])
    z = Add()([z_mu, z_eps])
    z = Lambda(lambda x: K.clip(x,-1,1))(z)

    x_pred =  model0(z) # model0

    z_mu = K.expand_dims(z_mu, axis = 1)
    z_log_var = K.expand_dims(z_log_var , axis = 1)

    z_mu0 = K.stop_gradient(z_mu)
    z_log_var0 = K.stop_gradient(z_log_var)
    z0 = K.stop_gradient(z)
    x0_pred =  model0(z0)

    z1 = Lambda(lambda x: x[:,:,0:1])(z)
    z2 = Lambda(lambda x: x[:,:,1:2])(z)

    z10 = K.stop_gradient(z1)
    z20 = K.stop_gradient(z2)

    nz, log_det = pm0([z1,z2])
    nz0, log_det0 = pm0([z10,z20])


    vae_loss = LossLayer(name='LossLayer')([z_mu0, z_log_var0, z, x_pred, x1, y1, nz, log_det])  #
    weight = WeightLayer(trainable = False,name='WeightLayer')(vae_loss) 

    output = Concatenate()([z, x_pred, nz, log_det])
    vae = Model(inputs=[x,x1, y1,eps], outputs=output) # batch * MC * (latent_dim + 1)

      
    def mise2(yTrue, yPred):
      var_y = K.sum(K.square(yTrue[:,:,latent_dim:(latent_dim+1)]- yPred[:,:,latent_dim:(latent_dim+1)]), axis=-1)

      return K.mean(K.sum(var_y * weight, axis = 1))
      

    def customLoss(yTrue, yPred):
      loss = K.sum(vae_loss * K.square(weight), axis = 1) 

      reconstruction_loss0 =  (K.sum(K.square(yTrue[:,:,latent_dim:(latent_dim+1)]- x0_pred), axis=-1)) /noisey/2
      reconstruction_loss0 += K.sum(K.square(nz0), axis = -1) /2 - K.sum(log_det0, axis = -1) #prior_loss0 
    
      reconstruction_loss0 = K.sum(reconstruction_loss0 * (weight - K.square(weight)), axis = 1) 
    
      if laplace:      
          return K.mean(loss + reconstruction_loss0, axis = 0) + K.log(noise)*latent_dim + K.log(noisey)/2
      else:
          return K.mean(loss + reconstruction_loss0, axis = 0) + K.log(noise)*latent_dim/2 + K.log(noisey)/2

    vae.compile(optimizer='adam', loss=customLoss, metrics = [mise2]) 
    history = vae.fit(
      [X_train[:,latent_dim:(latent_dim*2+1)],X_train[:,latent_dim:(latent_dim*2)],X_train[:,latent_dim*2]],
      np.expand_dims(X_train[:,latent_dim:(latent_dim*2+1)], axis=1),
      shuffle=True,
      epochs=epochs,
      verbose = 0, 
      batch_size=np.min([batch_size,X_train.shape[0]]),
      validation_data=(
          [X_val[:,latent_dim:(latent_dim*2+1)],X_val[:,latent_dim:(latent_dim*2)],X_val[:,latent_dim*2]],
          np.expand_dims(X_val[:,latent_dim:(latent_dim*2+1)], axis=1)
      ),
      callbacks=[noiseparam ]
    )
 
    #vae.summary() 

    model0_predict = model0.predict(test_dat[:,0:(latent_dim)])

    results[i, 6] += np.mean((model0_predict.transpose() - test_dat[:,latent_dim])**2)
    results[i, 5] += history.history['val_loss'][-1]
    results[i, 4] += history.history['loss'][-1]
    results[i, 3] += history.history['val_mise2'][-1]
    results[i, 2] += history.history['mise2'][-1]


  if repeat == '1':
    filename = prefix + "_learn_priorx_" + repeat + "_" + str(mc_samples)+ "_" + str(fold_size) +"_" + str(layer)+"_" +str(nodes) + "_" + str(layer2)+"_" +str(nodes2)+"_"+suffix+"_" + sys.argv[9]

    np.savetxt(filename + "_predict0.txt", np.squeeze(model00_predict))
    np.savetxt(filename + "_predict.txt", np.squeeze(model0_predict))


filename = prefix + "_learn_priorx_" + repeat + "_" + str(mc_samples) +"_" + str(layer)+"_" +str(nodes) + "_" + str(layer2)+"_" +str(nodes2) + "_" + suffix + "_" + sys.argv[9]
np.savetxt(filename + ".txt", results)


