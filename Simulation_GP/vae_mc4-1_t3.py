# -*- coding: utf-8 -*-
"""vae_mc3.ipynb
0425: change loss function (*latent_dim)
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_LKG42SnWDfjyqqhn-QKF76y4AyrGBcD
"""

import numpy as np
from keras.layers import Input, Dense, Lambda, Concatenate, Layer, Add, Multiply
from keras.models import Model, Sequential
from keras.optimizers import adam
from keras.losses import mse, mae
#from sklearn.model_selection import train_test_split #cross_validation
#from sklearn import preprocessing
#import pandas as pd
from keras import backend as K
#import matplotlib.pyplot as plt
#from functools import partial
from keras.callbacks import Callback
from keras import regularizers

## read in parameter
import sys
mc_samples = int(sys.argv[1])
#N = int(sys.argv[2])
#random_state = int(sys.argv[3])
suffix = sys.argv[2] #"1_16_0.2_0.1_1"
prefix = sys.argv[3] # "simulation_GP"
layer = int(sys.argv[4]) #7
nodes = int(sys.argv[5]) #32
layer2 = int(sys.argv[6]) #3
nodes2 = int(sys.argv[7]) #16
repeat = sys.argv[8] # 1-5
noise = float(sys.argv[9])**2
log_sigma2 = np.log(noise)

batch_size = 512
epochs = 400
epsilon_std = 1.0
#noise = 0.1**2 # for x
prior_mean = 0
prior_var = 0.5
regu = 1e-5

results = np.zeros((5, 7)) # NN_val_err, NN_test_ise, train_ise, val_ise, train_ll, val_ll, test_ise

file1 = prefix + "_train_" + suffix + "_" + sys.argv[9] + "_" + sys.argv[8] + ".txt"
file2 = prefix + "_test_" + suffix + "_0.1_" + sys.argv[8] + ".txt"
latent_dim = 2

train_dat = np.loadtxt(file1) #simulation_GP_train_1_16_0.2_0.1.txt
test_dat = np.loadtxt(file2) #np.concatenate((train_dat[2500:,0:latent_dim], train_dat[2500:, (2*latent_dim+1): (2*latent_dim+2)]), axis=1)#
idx = (test_dat[:,0] > 0.2) * (test_dat[:,1] < -0.2)!=1
test_dat = test_dat[idx,:]

idx = (test_dat[:,0] < -0.5) * (test_dat[:,1] > 0.5)!=1
test_dat = test_dat[idx,:]

def build_model(layer, nodes, activ ='relu', input_dim = 1, output_dim = 1, regu = -1, alpha = 0.3):
    model = Sequential()
    if regu > 0:
        model.add(Dense(nodes, input_dim=input_dim, activation=activ, 
                        kernel_regularizer=regularizers.l2(regu), bias_regularizer=regularizers.l2(regu))) #, kernel_initializer='he_normal', bias_initializer='he_normal'))
        for l in np.arange(layer):
            model.add(Dense(nodes, input_dim=nodes, activation=activ, 
                            kernel_regularizer=regularizers.l2(regu), bias_regularizer=regularizers.l2(regu)))#, kernel_initializer='he_normal', bias_initializer='he_normal'))
        model.add(Dense(output_dim, input_dim=nodes, kernel_regularizer=regularizers.l2(regu)))
    elif activ == 'leakyrelu':
        model.add(Dense(nodes, input_dim=input_dim))
        model.add(LeakyReLU(alpha = alpha))
        for l in np.arange(layer):
            model.add(Dense(nodes, input_dim=nodes))
            model.add(LeakyReLU(alpha = alpha))
        model.add(Dense(output_dim, input_dim=nodes))
    else:
        model.add(Dense(nodes, input_dim=input_dim, activation=activ)) 
        for l in np.arange(layer):
            model.add(Dense(nodes, input_dim=nodes, activation=activ))
        model.add(Dense(output_dim, input_dim=nodes))
   
    return model


class changeNoise(Callback):
  def __init__(self, noisey):
      super(changeNoise, self).__init__()
      self.noisey = noisey 

  def on_epoch_end(self, epoch, logs={}):
      #print("Setting noisey to =", str(K.get_value(self.noisey)))
      if epoch > 50:   
          K.set_value(self.noisey, logs.get('mise2'))

for i, fold_size in enumerate([125, 250, 500, 1000, 2000]): # [50, 125, 250, 500] for mixed kernel
  #X_train, _ = train_test_split(train_dat, test_size=train_dat.shape[0] - N, random_state=i)
  #X_train, X_val = train_test_split(train_dat[0:N,:], test_size=0.10,random_state=i)
  #cross validation:
  for f in [4]: #np.arange(5):
    X_train = np.concatenate((train_dat[0: f*fold_size,:], train_dat[(f+1)*fold_size:5*fold_size,:]))
    X_val = train_dat[f*fold_size : (f+1)*fold_size,:]
    
    val_err = 1e8
    # initialization by direct regression model (repeat 3 times, get the smallest validation error)
    for kk in np.arange(5):
      model0 = build_model(layer, nodes, input_dim = latent_dim, regu = regu)
      model0.compile(loss=mse,optimizer='adam')  

      history = model0.fit( X_train[:,latent_dim : (2*latent_dim)] , X_train[:,2*latent_dim],
          batch_size=np.min([1024,X_train.shape[0]]),epochs=500,verbose=0, shuffle=True)

      temp = np.mean((model0.predict(X_val[:,latent_dim : (2*latent_dim)]).transpose() - X_val[:,2*latent_dim])**2) #history.history['loss'][-1]
      if temp < val_err:
          val_err = temp
          org_weights = model0.get_weights()
    
    model0.set_weights(org_weights)
    #results[i, 1] = history.history['val_loss'][-1]
    results[i, 0] += val_err #np.mean((model0.predict(X_val[:,latent_dim : (2*latent_dim)]).transpose() - X_val[:,2*latent_dim])**2) #history.history['loss'][-1]


    model00_predict = model0.predict(test_dat[:,0:latent_dim])
    results[i, 1] += np.mean((model00_predict.transpose() - test_dat[:,latent_dim])**2)

    x = Input(shape=(latent_dim + 1,))
    x1 = Input(shape=(latent_dim,))
    y1 = Input(shape=(1,))

    #h = Dense(nodes2, activation='relu')(x)

    model_mu = build_model(layer2, nodes2, regu = regu, input_dim = latent_dim+1, output_dim = latent_dim) #input_dim = nodes2, 
    z_mu = model_mu(x) #h
    z_mu = Add()([z_mu, x1])

    model_var = build_model(layer2, nodes2, regu = regu, input_dim = latent_dim+1, output_dim = latent_dim)
    z_log_var = model_var(x)#h
    #z_log_var = Lambda(lambda t: t + log_sigma2)(z_log_var)

    encoder = Model([x,x1], [z_mu,z_log_var])

    z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)

    eps = Input(tensor=K.random_normal(stddev=epsilon_std,
                                       shape=(K.shape(x)[0],
                                              mc_samples,
                                              latent_dim)))
    z_eps = Multiply()([z_sigma, eps])
    z = Add()([z_mu, z_eps])


  # decoder = Sequential([
  #     Dense(32, input_dim=latent_dim, activation='tanh'),
  #     Dense(32, input_dim=32, activation='tanh'),
  #     Dense(32, input_dim=32, activation='tanh'),
  #     Dense(32, input_dim=32, activation='tanh'),
  #     Dense(1, activation='linear')
  # ])

    x_pred =  model0(z) # model0


   
    z_mu = K.expand_dims(z_mu, axis = 1)
    z_log_var = K.expand_dims(z_log_var , axis = 1)

    z_mu0 = K.stop_gradient(z_mu)
    z_log_var0 = K.stop_gradient(z_log_var)
    z0 = K.stop_gradient(z)
    x0_pred =  model0(z0)

    output = Concatenate()([z, x_pred])
    vae = Model(inputs=[x,x1, y1,eps], outputs=output) # batch * MC * (latent_dim + 1)

    # def weight_entropy(yTrue, yPred):
    #   return K.max(K.sum(weights * K.log(weights), axis = 1)) #K.max(weights) - K.min(weights) #K.mean(z_log_var)

    #def mise(yTrue, yPred):
        #return mse(decoder0(x1), yTrue[:,:,latent_dim:(latent_dim+1)])  # this is for compute true MISE
        #return mse(model0(z_mu0), yTrue[:,:,latent_dim:(latent_dim+1)]) #0:1
      
    def mise2(yTrue, yPred):
      var_y = K.sum(K.square(yTrue[:,:,latent_dim:(latent_dim+1)]- yPred[:,:,latent_dim:(latent_dim+1)]), axis=-1)
      reconstruction_loss = var_y/noisey + K.sum(K.square(yTrue[:,:,0:latent_dim]- yPred[:,:,0:latent_dim]), axis=-1) /noise
      reconstruction_loss /= 2
    
      prior_loss = 1.5 * K.log(1 + K.square(z - prior_mean)/prior_var/2) # v = 2
      prior_loss = K.sum(prior_loss, axis=-1)
   

      post_loss = .5 * (K.square(z_mu0 - z) /K.exp(z_log_var0) + z_log_var0)
      post_loss = K.sum(post_loss, axis=-1)

      vae_loss = reconstruction_loss + prior_loss - post_loss

      log_weight = K.stop_gradient(-vae_loss)
      log_weight -= K.max(log_weight,axis = 1,keepdims= True)

      weight = K.exp(log_weight)
      weight = weight/K.sum(weight,axis = 1,keepdims= True)

      return K.mean(K.sum(var_y * weight, axis = 1))
      
            

    noisey =  K.variable(0.1)
    noiseparam = changeNoise(noisey)

    def customLoss(yTrue, yPred):
      #reconstruction_loss =  (K.sum(K.square(yTrue- yPred), axis=-1)) /noise/2
      reconstruction_loss = K.sum(K.square(yTrue[:,:,latent_dim:(latent_dim+1)]- yPred[:,:,latent_dim:(latent_dim+1)]), axis=-1)/noisey
      reconstruction_loss += K.sum(K.square(yTrue[:,:,0:latent_dim]- yPred[:,:,0:latent_dim]), axis=-1) /noise
      reconstruction_loss /= 2

      prior_loss = 1.5 * K.log(1 + K.square(z - prior_mean)/prior_var/2) # v = 2
      prior_loss = K.sum(prior_loss, axis=-1)
     
          
      post_loss = .5 * (K.square(z_mu0 - z) /K.exp(z_log_var0) + z_log_var0)
      post_loss = K.sum(post_loss, axis=-1)
      
      vae_loss = reconstruction_loss + prior_loss - post_loss
      
      
      log_weight = K.stop_gradient(-vae_loss)
      log_weight -= K.max(log_weight,axis = 1,keepdims= True)
            
      weight = K.exp(log_weight)
      weight = weight/K.sum(weight,axis = 1,keepdims= True)
      
      vae_loss = K.sum(vae_loss * K.square(weight), axis = 1) 
      
      reconstruction_loss0 =  (K.sum(K.square(yTrue[:,:,latent_dim:(latent_dim+1)]- x0_pred), axis=-1)) /noisey/2
      reconstruction_loss0 = K.sum(reconstruction_loss0 * (weight - K.square(weight)), axis = 1) 
            
      return K.mean(vae_loss + reconstruction_loss0, axis = 0) + 0.5* np.log(noise) * latent_dim + 0.5*K.log(noisey)

    ada = adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.004, amsgrad=False)
    vae.compile(optimizer=ada, loss=customLoss, metrics = [mise2]) #rmsprop, 
    #vae.summary() 



    history = vae.fit(
      [X_train[:,latent_dim:(latent_dim*2+1)],X_train[:,latent_dim:(latent_dim*2)],X_train[:,latent_dim*2]],
      np.expand_dims(X_train[:,latent_dim:(latent_dim*2+1)], axis=1),
      shuffle=True,
      epochs=epochs,
      verbose = 0, 
      batch_size=np.min([batch_size,X_train.shape[0]]),
      validation_data=(
          [X_val[:,latent_dim:(latent_dim*2+1)],X_val[:,latent_dim:(latent_dim*2)],X_val[:,latent_dim*2]],
          np.expand_dims(X_val[:,latent_dim:(latent_dim*2+1)], axis=1)
      ),
      callbacks=[noiseparam ]
    )


    model0_predict = model0.predict(test_dat[:,0:(latent_dim)])

    results[i, 6] += np.mean((model0_predict.transpose() - test_dat[:,latent_dim])**2)
    results[i, 5] += history.history['val_loss'][-1]
    results[i, 4] += history.history['loss'][-1]
    results[i, 3] += history.history['val_mise2'][-1]
    results[i, 2] += history.history['mise2'][-1]

  #results[i, 2] = np.mean((model0.predict(X_train[:,latent_dim:(latent_dim*2)]).transpose() - X_train[:,(latent_dim*2)])**2)

  
  # # deterministic query-time encoder
  #predict_x = encoder.predict(X_train[:,latent_dim:(latent_dim*2+1)])

  if repeat == '1':
    filename = prefix + "_dpar_" + repeat + "_" + str(mc_samples)+ "_" + str(fold_size) +"_" + str(layer)+"_" +str(nodes) + "_" + str(layer2)+"_" +str(nodes2)+"_"+suffix+"_" + sys.argv[9]
#    fd = int(np.sqrt(test_dat.shape[0]))

#    vmi = np.min(test_dat[:,latent_dim])
#    vma = np.max(test_dat[:,latent_dim])
#    fig, (ax1, ax2, ax3, ax4) = plt.subplots(figsize=(17, 3), ncols=4)
#    pos = ax1.imshow(np.reshape(test_dat[:,latent_dim], (fd,fd)), cmap='RdBu' , vmin=vmi, vmax=vma)
#    ax1.set_title("test data")
#    fig.colorbar(pos, ax=ax1)
#
#
#    pos = ax2.imshow(np.reshape(model0_predict.transpose(), (fd,fd)),cmap='RdBu', vmin=vmi, vmax=vma)
#    ax2.set_title("VAE predict")
#    fig.colorbar(pos, ax=ax2)
#
#    pos = ax3.imshow(np.reshape(model00_predict.transpose(), (fd,fd)),cmap='RdBu', vmin=vmi, vmax=vma)
#    ax3.set_title("NN predict")
#    fig.colorbar(pos, ax=ax3)
#
#    pos = ax4.imshow(np.reshape(model0_predict.transpose() - model00_predict.transpose(), (fd,fd)),cmap='RdBu')
#    ax4.set_title("VAE - NN")
#    fig.colorbar(pos, ax=ax4)
#
#
#    fig.savefig(filename + "_function.png")
    np.savetxt(filename + "_predict0.txt", np.squeeze(model00_predict))
    np.savetxt(filename + "_predict.txt", np.squeeze(model0_predict))

  #plt.figure(figsize=(8,8))
  #plt.scatter(test_dat[:,0], test_dat[:,1], color='C1',s=3)
  #plt.plot(test_dat[:,0], model0_predict)
  #plt.plot(test_dat[:,0], model00_predict)
  #plt.savefig(filename + "_prediction.png")

filename = prefix + "_dpar_" + repeat + "_" + str(mc_samples) +"_" + str(layer)+"_" +str(nodes) + "_" + str(layer2)+"_" +str(nodes2) + "_" + suffix + "_" + sys.argv[9]
np.savetxt(filename + ".txt", results)


